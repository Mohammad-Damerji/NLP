{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AYKN6P NLP Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYxzOUmZ1oGZ"
      },
      "source": [
        "## Import libraries and include the Brown data ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN52k9Wcqp61",
        "outputId": "8b2b4d51-bf14-4f41-b2a7-f8352676801c"
      },
      "source": [
        "!pip install sklearn-crfsuite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (0.3.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.8.9)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (0.9.7)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite) (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W0-G9EnDchF",
        "outputId": "7fabef1f-6d02-4d5d-a471-6f7604aee803"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBxlbsh92nhD"
      },
      "source": [
        "## The function (token2features) for the feature extraction.\n",
        "* The input is list of tokens.\n",
        "* The output is list of dictionary which contains the features of the current, the previous and the next word for every word as a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9quz3KXKELoe"
      },
      "source": [
        "def token2features(ListOfWords):\n",
        "    doc = Doc(nlp.vocab, words=ListOfWords)\n",
        "    result = []\n",
        "    I = range(len(doc))\n",
        "    for i  in I:\n",
        "      features = {\n",
        "          'lower': doc[i].lower_,\n",
        "          'suffix': doc[i].suffix_,\n",
        "          'prefix': doc[i].prefix_,\n",
        "          'isupper': doc[i].is_upper,\n",
        "          'istitle': doc[i].is_title,\n",
        "          'isdigit': doc[i].is_digit\n",
        "          }\n",
        "      if i > 0:\n",
        "          features.update({\n",
        "              '-1_lower': doc[i-1].lower_,\n",
        "              '-1_suffix': doc[i-1].suffix_,\n",
        "              '-1_prefix': doc[i-1].prefix_,\n",
        "              '-1_isupper': doc[i-1].is_upper,\n",
        "              '-1_istitle': doc[i-1].is_title,\n",
        "              '-1_isdigit': doc[i-1].is_digit\n",
        "          })\n",
        "      else:\n",
        "          features['BOS'] = True\n",
        "\n",
        "      if i < len(doc)-1:\n",
        "          features.update({\n",
        "              '+1_lower': doc[i+1].lower_,\n",
        "              '+1_suffix': doc[i+1].suffix_,\n",
        "              '+1_prefix': doc[i+1].prefix_,\n",
        "              '+1_isupper': doc[i+1].is_upper,\n",
        "              '+1_istitle': doc[i+1].is_title,\n",
        "              '+1_isdigit': doc[i+1].is_digit,\n",
        "          })\n",
        "      else:\n",
        "          features['EOS'] = True\n",
        "\n",
        "      result.append(features)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBjIYF9g3yRa"
      },
      "source": [
        "## I assign the Brown tagged sentences with the universal tagset for the variable \"sentences\".\n",
        "## I assign the Brown sentences for the variable \"sentences1\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR-3nBUAEpx2"
      },
      "source": [
        "sentences = brown.tagged_sents(tagset=\"universal\")\n",
        "sentences1 = brown.sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wsWB32t4qzb"
      },
      "source": [
        "## I use list comprehension with token2features function to get list of lists of dictionaries.\n",
        "## I use list comprehension to get list of lists of strings where those strings are POS tags for each word in Brown data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A38Qmo3FEs5N"
      },
      "source": [
        "sents = [token2features(sent) for sent in sentences1]\n",
        "labels = [[s[1] for s in lis] for lis in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrcHmr0f62gF"
      },
      "source": [
        "## Now we have the input data in \"sents\" variable and the output data in \"labels\" variable, I split them to train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDBjTLJREwmM"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(sents,labels, test_size=0.1,shuffle=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXHIuErTE0CT",
        "outputId": "e749bf10-283c-4b87-a4e3-5b840b462240"
      },
      "source": [
        "len(X_train) , len(y_train) , len(X_test) , len(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51606, 51606, 5734, 5734)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KssWKmNF7YZr"
      },
      "source": [
        "## I create a CRF model using sklearn_crfsuite.\n",
        "## I fit the model with train data.\n",
        "* Conditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account.(From Wikipedia)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcYgEmOKE2UF",
        "outputId": "03e2887a-0477-48d9-ca99-cdbee7a245b8"
      },
      "source": [
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=True,\n",
              "    averaging=None, c=None, c1=0.1, c2=0.1, calibration_candidates=None,\n",
              "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
              "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
              "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
              "    max_linesearch=None, min_freq=None, model_filename=None, num_memories=None,\n",
              "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odU-pSWK9cEC"
      },
      "source": [
        "## I show the unique labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puCsY8ozE5cz",
        "outputId": "73e27590-0e9f-45a8-d411-3c41ac3f730d"
      },
      "source": [
        "labels = list(crf.classes_)\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DET',\n",
              " 'NOUN',\n",
              " 'ADJ',\n",
              " 'VERB',\n",
              " 'ADP',\n",
              " '.',\n",
              " 'ADV',\n",
              " 'CONJ',\n",
              " 'PRT',\n",
              " 'PRON',\n",
              " 'NUM',\n",
              " 'X']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rQYeLpB-HMh"
      },
      "source": [
        "## I use the last model to make prediction with test data, we get a good score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF6uBGWx3yjo",
        "outputId": "c2f738ab-ae76-4aa6-9b06-a81b54598c83"
      },
      "source": [
        "y_pred = crf.predict(X_test)\n",
        "metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9744252902116789"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PZkc8HT31Nm",
        "outputId": "9bdfeb9b-e012-4f5d-dca1-7e60a530cee8"
      },
      "source": [
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           .      1.000     1.000     1.000     15505\n",
            "           X      0.829     0.234     0.365       124\n",
            "         ADJ      0.916     0.907     0.912      5492\n",
            "         ADP      0.971     0.977     0.974      9630\n",
            "         ADV      0.941     0.937     0.939      5357\n",
            "        VERB      0.982     0.981     0.982     16917\n",
            "         DET      0.991     0.989     0.990     10113\n",
            "        CONJ      0.991     0.999     0.995      3326\n",
            "        NOUN      0.965     0.973     0.969     17692\n",
            "        PRON      0.992     0.990     0.991      7353\n",
            "         PRT      0.954     0.924     0.939      3446\n",
            "         NUM      0.862     0.986     0.920       487\n",
            "\n",
            "    accuracy                          0.975     95442\n",
            "   macro avg      0.949     0.908     0.914     95442\n",
            "weighted avg      0.975     0.975     0.974     95442\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI_3VgGV36iT",
        "outputId": "9bde8464-3050-4c39-b419-868a28f2a5c8"
      },
      "source": [
        "y_pred[0:2] , y_test[0:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([['PRON', 'VERB', 'ADV', 'NUM', 'NOUN', 'ADJ', '.'],\n",
              "  ['.', 'DET', 'ADJ', 'NOUN', '.', 'DET', 'NOUN', '.', '.']],\n",
              " [['PRON', 'VERB', 'ADV', 'NUM', 'NOUN', 'ADJ', '.'],\n",
              "  ['.', 'DET', 'ADJ', 'NOUN', '.', 'DET', 'NOUN', '.', '.']])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ztZTJEe_PGS"
      },
      "source": [
        "## The funcion (pos_tagger):\n",
        "* The input is either untokenized string sentences or tokenized list of words.\n",
        "* The output is a list of POS tags\n",
        "* If the input is a string sentence, I use spacy for tokenization then I call token2features function for creating the features then I use the last model to pridict POS tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAJyZutqXI7K"
      },
      "source": [
        "def pos_tagger(sentence):\n",
        "    if type(sentence) == str:\n",
        "        doc = nlp(sentence)\n",
        "        sentence = [word.text for word in doc]\n",
        "    \n",
        "    creatingFeatures = token2features(sentence)\n",
        "    pred = crf.predict([creatingFeatures])\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_T8-dCBDv7"
      },
      "source": [
        "## Here you can input a text, the POS tags will be displaied using pos_tagger function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNOZ_zy0XJ1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5bc667-385a-4ebf-f1c1-102be84818f9"
      },
      "source": [
        "text = input('Input your text: ')\n",
        "print(\"\\n\",pos_tagger(text)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input your text: This is Mohammad\n",
            "\n",
            " ['DET', 'VERB', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kymGNA9Uv6fV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}