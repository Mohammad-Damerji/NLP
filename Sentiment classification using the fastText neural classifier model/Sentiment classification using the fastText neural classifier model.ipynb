{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import re\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.layers import Embedding , Activation, Dropout,  GlobalMaxPooling1D  \n",
    "from keras.layers import Conv1D , Input , Flatten, Dense "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford Large Movie Review dataset contains two files.\n",
    "* One for the training set and the other for the test set.\n",
    "* The training file contains two files one for the positive reviews which contains 12500 reviews and the other for the negative reviews which contains 12500 reviews and the same for test file.\n",
    "* Each review was built as a text file, so I created the function \"readData\" which reads text files and returns a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(directory):\n",
    "    lst = []\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # open the file as read only\n",
    "        file = open(path,'r',encoding=\"utf8\")\n",
    "        # read all text\n",
    "        text = file.read()\n",
    "        lst.append(text)\n",
    "        # close the file\n",
    "        file.close()\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "directoryOfTrainPos = r'E:\\2020-21-2\\NLP\\FourthProject\\aclImdb\\train\\pos'\n",
    "directoryOfTrainNeg = r'E:\\2020-21-2\\NLP\\FourthProject\\aclImdb\\train\\neg'\n",
    "directoryOfTestPos = r'E:\\2020-21-2\\NLP\\FourthProject\\aclImdb\\test\\pos'\n",
    "directoryOfTestNeg = r'E:\\2020-21-2\\NLP\\FourthProject\\aclImdb\\test\\neg'\n",
    "\n",
    "TrainPos = readData(directoryOfTrainPos)\n",
    "TrainNeg = readData(directoryOfTrainNeg)\n",
    "TestPos = readData(directoryOfTestPos)\n",
    "TestNeg = readData(directoryOfTestNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TrainPos) , len(TrainNeg) , len(TestPos) , len(TestNeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After reading the data, I merged the positive and negative training data and the same for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = TrainPos + TrainNeg\n",
    "testData = TestPos + TestNeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I created the output of training and test data where the number one means positive review and zero means negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [1]*len(TrainPos) + [0]*len(TrainNeg) \n",
    "Y_test =  [1]*len(TestPos) + [0]*len(TestNeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I created the function \"PreprocessingData\" to processing the data, it substitutes any strange string with ',' then I used Spacy to tokenize, lemmatize and lower the words of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Preprocessing\n",
    "def PreprocessingData(text):\n",
    "    review = re.sub('[^a-zA-Z]', ', ' , text)\n",
    "    doc = nlp(review)\n",
    "    review = [word.lemma_ for word in doc]\n",
    "    review = [word.lower() for word in review if not (word in all_stopwords) and len(word)>2]\n",
    "    review = ' '.join(review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = [PreprocessingData(text) for text in trainData]\n",
    "testData = [PreprocessingData(text) for text in testData]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I shuffled the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(zip(trainData, Y_train))\n",
    "test = list(zip(testData, Y_test))\n",
    "\n",
    "random.shuffle(train)\n",
    "random.shuffle(test)\n",
    "\n",
    "\n",
    "trainData, Y_train = zip(*train)\n",
    "testData, Y_test = zip(*test)\n",
    "\n",
    "trainData = list(trainData)\n",
    "testData = list(testData)\n",
    "Y_train = list(Y_train)\n",
    "Y_test = list(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I made the size of training data as 49000 and 1000 as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = trainData + testData[:24000]\n",
    "Y_train = Y_train + Y_test[:24000]\n",
    "\n",
    "testData = testData[24000:]\n",
    "Y_test = Y_test[24000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I saved the preprocessing training data as a CSV file and the same for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_Train_Of_Review = {'trainData': trainData , 'Y_train': Y_train }\n",
    "df_train = pd.DataFrame(data_Train_Of_Review, columns = ['trainData', 'Y_train'])\n",
    "df_train.to_csv('data_Train_Of_Review.csv', index=False)\n",
    "\n",
    "data_Test_Of_Review = {'testData':testData , 'Y_test': Y_test }\n",
    "df_test = pd.DataFrame(data_Test_Of_Review, columns = ['testData', 'Y_test'])\n",
    "df_test.to_csv('data_Test_Of_Review.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data_Train_Of_Review.csv\")\n",
    "df_test = pd.read_csv(\"data_Test_Of_Review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               trainData  Y_train\n",
      "0      jim henson muppet favorite childhood film feel...        1\n",
      "1      fun theater guilty pleasure corner movie taste...        1\n",
      "2      good way baseketball waste film single way off...        0\n",
      "3      amazing story film direct larry clark story sc...        1\n",
      "4      thomas ian griffith doesn polish big buck acto...        0\n",
      "...                                                  ...      ...\n",
      "48995  great british director christopher nolan momen...        1\n",
      "48996  new york attorney plot rid senile mother meeti...        0\n",
      "48997  film crap probably bad film advice don watch f...        0\n",
      "48998  ken russell direct weird erotic thriller hadn ...        0\n",
      "48999  wrap people enjoy film watch early teen time l...        1\n",
      "\n",
      "[49000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = [sentence for sentence  in df_train.loc[:, 'trainData']]\n",
    "texts_test = [sentence for sentence  in df_test.loc[:, 'testData']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [i for i  in df_train.loc[:, 'Y_train']]\n",
    "Y_test =  [i for i  in df_test.loc[:, 'Y_test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I made the classes as follow: \n",
    "* [ 1 , 0 ] for positive review. \n",
    "* [ 0 , 1 ] for negative review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array([np.array([1.,0.]) if i==1 else np.array([0.,1.]) for i in Y_train])\n",
    "Y_test = np.array([np.array([1.,0.]) if i==1 else np.array([0.,1.]) for i in Y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The input of the first model will be based on one hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=10000\n",
    "\n",
    "X_train_oneHot = [one_hot(sentence,max_features) for sentence in texts_train]\n",
    "X_test_oneHot  = [one_hot(sentence,max_features) for sentence in texts_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I decided to set 256 as the maximum length sentence and 50 as the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sent_length=256\n",
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pad_sequences function is used to ensure that all sequences in a list have the same length by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = pad_sequences(X_train_oneHot , padding='pre', maxlen=max_sent_length)\n",
    "X_test_pad = pad_sequences(X_test_oneHot , padding='pre', maxlen=max_sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0, 9956, 6439, 4967, 1677, 2127, 8925, 4783,\n",
       "        276, 1887, 2023, 9986, 1539, 6690, 4142, 1540,  275, 4415, 4925,\n",
       "       6123, 2881, 3048, 1513, 7631, 3466, 3048, 1513, 7631, 5316, 3469,\n",
       "       8925,   99, 8498, 4664, 1887, 2847, 8386, 4224, 8985, 9986,   90,\n",
       "       4744, 4142, 1539,   90, 2694, 3048, 1513, 7631, 8558, 4402, 1887,\n",
       "       3681, 7706,  276])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model:\n",
    "* Embedding layer\n",
    "* Two Pooling layers with GlobalMaxPool1D\n",
    "* Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A integer input for vocab indices.\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "x = layers.Embedding(max_features, embedding_dim)(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# I added a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "predictions = layers.Dense(2, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model_1 = keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model_1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1532/1532 [==============================] - 52s 34ms/step - loss: 0.5212 - accuracy: 0.7035\n",
      "Epoch 2/10\n",
      "1532/1532 [==============================] - 55s 36ms/step - loss: 0.2940 - accuracy: 0.8819\n",
      "Epoch 3/10\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 0.2340 - accuracy: 0.9078\n",
      "Epoch 4/10\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 0.1816 - accuracy: 0.9304\n",
      "Epoch 5/10\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 0.1500 - accuracy: 0.9440\n",
      "Epoch 6/10\n",
      "1532/1532 [==============================] - 55s 36ms/step - loss: 0.1218 - accuracy: 0.9530\n",
      "Epoch 7/10\n",
      "1532/1532 [==============================] - 55s 36ms/step - loss: 0.1055 - accuracy: 0.9601\n",
      "Epoch 8/10\n",
      "1532/1532 [==============================] - 55s 36ms/step - loss: 0.0969 - accuracy: 0.9637\n",
      "Epoch 9/10\n",
      "1532/1532 [==============================] - 54s 35ms/step - loss: 0.0845 - accuracy: 0.9691\n",
      "Epoch 10/10\n",
      "1532/1532 [==============================] - 55s 36ms/step - loss: 0.0710 - accuracy: 0.9743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29ba05bfb50>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "model_1.fit(X_train_pad, Y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 50)          500000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         44928     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 676,514\n",
      "Trainable params: 676,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = model_1.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.9998105e-01, 1.9062576e-05],\n",
       "        [4.5256755e-05, 9.9996829e-01],\n",
       "        [9.9959350e-01, 4.4307113e-04],\n",
       "        [2.8655586e-05, 9.9998349e-01]], dtype=float32),\n",
       " array([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1[1:5] , Y_test[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 7ms/step - loss: 0.4822 - accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4821602702140808, 0.8500000238418579]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(X_test_pad, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The input of the second model will be based on BPEmbed pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "bpemb_en = BPEmb(lang='en' , dim = embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainData_bpemb = [bpemb_en.embed(text) for text in texts_train]\n",
    "TestData_bpemb = [bpemb_en.embed(text) for text in texts_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function \"sameDim\" is used to add zeros arrays if the list has less than max_sent_length arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sameDim(matrix):\n",
    "    result = matrix[:max_sent_length]\n",
    "    l = len(result)\n",
    "    if l<max_sent_length:\n",
    "        zeroArr = np.zeros((max_sent_length-l,50)).astype(np.float32)\n",
    "        result = np.concatenate((result, zeroArr), axis=0) \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData_bpemb = np.array([sameDim(matrix) for matrix in TrainData_bpemb])\n",
    "TestData_bpemb =  np.array([sameDim(matrix) for matrix in TestData_bpemb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.075245,  0.166705,  0.11528 , ...,  0.931   ,  0.79428 ,\n",
       "        -0.435135],\n",
       "       [ 0.835609,  0.218231,  0.148973, ...,  0.308579,  0.521739,\n",
       "        -0.584431],\n",
       "       [ 0.966539, -0.155275, -0.047727, ...,  0.452914,  0.28516 ,\n",
       "        -0.485385],\n",
       "       ...,\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ],\n",
       "       [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "         0.      ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainData_bpemb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second model:\n",
    "* Two Pooling layers with GlobalMaxPool1D\n",
    "* Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A integer input for vocab indices.\n",
    "inputs = keras.Input(shape=(None,embedding_dim))\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(inputs)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# I add a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "predictions = layers.Dense(2, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model_2 = keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model_2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1532/1532 [==============================] - 57s 31ms/step - loss: 0.5992 - accuracy: 0.6693\n",
      "Epoch 2/10\n",
      "1532/1532 [==============================] - 41s 27ms/step - loss: 0.4686 - accuracy: 0.7785\n",
      "Epoch 3/10\n",
      "1532/1532 [==============================] - 41s 27ms/step - loss: 0.3764 - accuracy: 0.8322\n",
      "Epoch 4/10\n",
      "1532/1532 [==============================] - 42s 27ms/step - loss: 0.2871 - accuracy: 0.8797\n",
      "Epoch 5/10\n",
      "1532/1532 [==============================] - 42s 27ms/step - loss: 0.2042 - accuracy: 0.9189\n",
      "Epoch 6/10\n",
      "1532/1532 [==============================] - 44s 29ms/step - loss: 0.1437 - accuracy: 0.9439\n",
      "Epoch 7/10\n",
      "1532/1532 [==============================] - 43s 28ms/step - loss: 0.0989 - accuracy: 0.9634\n",
      "Epoch 8/10\n",
      "1532/1532 [==============================] - 42s 27ms/step - loss: 0.0810 - accuracy: 0.9698\n",
      "Epoch 9/10\n",
      "1532/1532 [==============================] - 44s 28ms/step - loss: 0.0721 - accuracy: 0.9740\n",
      "Epoch 10/10\n",
      "1532/1532 [==============================] - 43s 28ms/step - loss: 0.0556 - accuracy: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29bb54b5a90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "model_2.fit(TrainData_bpemb, Y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, None, 50)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 128)         44928     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 128)         114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 176,514\n",
      "Trainable params: 176,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2 = model_2.predict(TestData_bpemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.2296519e-03, 9.9487925e-01],\n",
       "        [1.4853650e-01, 8.3828413e-01],\n",
       "        [1.8583834e-03, 9.9821222e-01],\n",
       "        [3.0855579e-10, 1.0000000e+00]], dtype=float32),\n",
       " array([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2[1:5] , Y_test[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 9ms/step - loss: 1.0083 - accuracy: 0.7650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0082924365997314, 0.7649999856948853]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(TestData_bpemb, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I created a list that contains the unique words from training and test data to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_review_data = []\n",
    "\n",
    "for lst in texts_train:\n",
    "    doc = nlp(lst)\n",
    "    lst_split = [word.text for word in doc]\n",
    "    for word in lst_split:\n",
    "        if not word in dic_review_data:\n",
    "            dic_review_data.append(word)\n",
    "\n",
    "for lst in texts_test:\n",
    "    doc = nlp(lst)\n",
    "    lst_split = [word.text for word in doc]\n",
    "    for word in lst_split:\n",
    "        if not word in dic_review_data:\n",
    "            dic_review_data.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The input of the third model will be based on pre-trained fastText embeddings.\n",
    "* I downloaded the model \"wiki-news-300d-1M.vec\" from this page https://fasttext.cc/docs/en/english-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fasttext = io.open('wiki-news-300d-1M.vec', 'r', encoding='utf-8', newline='\\n', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataOf_fastText = model_fasttext.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I created a dictionary from the pre-trained model, this dictionary contains the same words which exist in our data (dic_review_data) as indexes where the values are vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_fastText = {}\n",
    "\n",
    "for item in dataOf_fastText:\n",
    "    doc = nlp(item)\n",
    "    item_split = [word.text for word in doc]\n",
    "    if item_split[0] in dic_review_data:\n",
    "        vector = [float(num) for num in item_split[1:embedding_dim+1]]\n",
    "        dic_fastText[item_split[0]] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The size of fastText model is very big so I saved the last dictionary as CSV file to avoid loading the model each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "vectors = []\n",
    "for word,vec in dic_fastText.items():\n",
    "    words.append(word)\n",
    "    vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fastText = {'words': words , 'vectors': vectors }\n",
    "df_data_fastText = pd.DataFrame(data_fastText, columns = ['words', 'vectors'])\n",
    "df_data_fastText.to_csv('df_data_fastText.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_fastText = pd.read_csv('df_data_fastText.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>[0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that</td>\n",
       "      <td>[0.0806, -0.0063, 0.0875, 0.0152, -0.068, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>with</td>\n",
       "      <td>[0.0177, -0.0273, -0.0135, 0.0351, -0.0135, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>was</td>\n",
       "      <td>[0.0986, 0.0069, -0.0897, -0.0036, 0.0114, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this</td>\n",
       "      <td>[-0.038, -0.0383, -0.0304, -0.0533, -0.0059, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  words                                            vectors\n",
       "0   the  [0.0897, 0.016, -0.0571, 0.0405, -0.0696, -0.1...\n",
       "1  that  [0.0806, -0.0063, 0.0875, 0.0152, -0.068, -0.0...\n",
       "2  with  [0.0177, -0.0273, -0.0135, 0.0351, -0.0135, -0...\n",
       "3   was  [0.0986, 0.0069, -0.0897, -0.0036, 0.0114, -0....\n",
       "4  this  [-0.038, -0.0383, -0.0304, -0.0533, -0.0059, -..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_fastText.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word  in df_data_fastText.loc[:, 'words']]\n",
    "vectors = [vector for vector  in df_data_fastText.loc[:, 'vectors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_fastText ={}\n",
    "for i in range(len(words)):\n",
    "    vec = vectors[i].split(',')\n",
    "    firstNum = [float(vec[0][1:])]\n",
    "    lastNum = [float(vec[-1][:-1])]\n",
    "    vec_numbers = firstNum + [float(i) for i in vec[1:-1]] + lastNum\n",
    "    dic_fastText[words[i]] = vec_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0897,\n",
       " 0.016,\n",
       " -0.0571,\n",
       " 0.0405,\n",
       " -0.0696,\n",
       " -0.1237,\n",
       " 0.0301,\n",
       " 0.0248,\n",
       " -0.0303,\n",
       " 0.0174,\n",
       " 0.0063,\n",
       " 0.0184,\n",
       " 0.0217,\n",
       " -0.0257,\n",
       " 0.035,\n",
       " -0.0242,\n",
       " 0.0029,\n",
       " 0.0188,\n",
       " -0.057,\n",
       " 0.0252,\n",
       " -0.021,\n",
       " -0.0008,\n",
       " 0.036,\n",
       " -0.0729,\n",
       " -0.0665,\n",
       " 0.0989,\n",
       " 0.0676,\n",
       " 0.0852,\n",
       " -0.0089,\n",
       " 0.0313,\n",
       " -0.0069,\n",
       " -0.0032,\n",
       " -0.0462,\n",
       " 0.0497,\n",
       " 0.0261,\n",
       " 0.0268,\n",
       " -0.031,\n",
       " -0.1361,\n",
       " -0.0062,\n",
       " 0.0375,\n",
       " -0.032,\n",
       " -0.0106,\n",
       " 0.0534,\n",
       " -0.0187,\n",
       " 0.0638,\n",
       " 0.0094,\n",
       " 0.0047,\n",
       " -0.053,\n",
       " 0.0093,\n",
       " -0.0087]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_fastText['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function \"getVectors\":\n",
    "* Input: a sentence as string.\n",
    "* Output: for each word, it gets the vector from the pre-trained fastText model (from the dictionary dic_fastText) and returns a list of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVectors(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    sentence_split = [word.text for word in doc]\n",
    "    vectors = []\n",
    "    for word in sentence_split:\n",
    "        if word in dic_fastText:\n",
    "            vectors.append(np.array(dic_fastText[word]))\n",
    "        else:\n",
    "            vectors.append(np.array([0.]*embedding_dim))\n",
    "    return np.array(vectors).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainData_fastText = [getVectors(sentence) for sentence in texts_train]\n",
    "TestData_fastText =  [getVectors(sentence) for sentence in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData_fastText = np.array([sameDim(matrix) for matrix in TrainData_fastText]).astype(np.float32)\n",
    "TestData_fastText =  np.array([sameDim(matrix) for matrix in TestData_fastText]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0054,  0.1111, -0.1341, ..., -0.0126, -0.0774, -0.1184],\n",
       "       [-0.0114,  0.1345, -0.024 , ..., -0.0205, -0.0457, -0.0546],\n",
       "       [-0.0183,  0.256 , -0.1479, ..., -0.0689,  0.0284, -0.1965],\n",
       "       ...,\n",
       "       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainData_fastText[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third model:\n",
    "* Two Pooling layers with GlobalMaxPool1D\n",
    "* Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A integer input for vocab indices.\n",
    "inputs = keras.Input(shape=(None,embedding_dim))\n",
    "\n",
    "# Conv1D + global max pooling\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(inputs)\n",
    "x = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "# I added a vanilla hidden layer:\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "predictions = layers.Dense(2, activation=\"sigmoid\", name=\"predictions\")(x)\n",
    "\n",
    "model_3 = keras.Model(inputs, predictions)\n",
    "\n",
    "# Compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model_3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1532/1532 [==============================] - 45s 25ms/step - loss: 0.5668 - accuracy: 0.6879\n",
      "Epoch 2/10\n",
      "1532/1532 [==============================] - 36s 24ms/step - loss: 0.3864 - accuracy: 0.8287\n",
      "Epoch 3/10\n",
      "1532/1532 [==============================] - 37s 24ms/step - loss: 0.2729 - accuracy: 0.8891\n",
      "Epoch 4/10\n",
      "1532/1532 [==============================] - 39s 25ms/step - loss: 0.1770 - accuracy: 0.93270s - loss: 0.1769 - accura\n",
      "Epoch 5/10\n",
      "1532/1532 [==============================] - 37s 24ms/step - loss: 0.1043 - accuracy: 0.9629\n",
      "Epoch 6/10\n",
      "1532/1532 [==============================] - 37s 24ms/step - loss: 0.0673 - accuracy: 0.9756\n",
      "Epoch 7/10\n",
      "1532/1532 [==============================] - 37s 24ms/step - loss: 0.0526 - accuracy: 0.9809\n",
      "Epoch 8/10\n",
      "1532/1532 [==============================] - 37s 24ms/step - loss: 0.0430 - accuracy: 0.9849\n",
      "Epoch 9/10\n",
      "1532/1532 [==============================] - 37s 24ms/step - loss: 0.0383 - accuracy: 0.9870\n",
      "Epoch 10/10\n",
      "1532/1532 [==============================] - 37s 24ms/step - loss: 0.0411 - accuracy: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29b8f3258e0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "model_3.fit(TrainData_fastText, Y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, None, 50)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 128)         44928     \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 128)         114816    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 176,514\n",
      "Trainable params: 176,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = model_3.predict(TestData_fastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.0000000e+00, 8.4444531e-09],\n",
       "        [4.5901537e-04, 9.9954885e-01],\n",
       "        [1.6003827e-07, 1.0000000e+00],\n",
       "        [2.5254488e-04, 9.9975002e-01]], dtype=float32),\n",
       " array([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_3[1:5] , Y_test[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 8ms/step - loss: 0.9825 - accuracy: 0.8020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9825292229652405, 0.8019999861717224]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(TestData_fastText, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison:\n",
    "I got the following results:\n",
    "* First model:-----------------------------accuracy 0.9743 , evaluation of test data [0.4821, 0.8500]\n",
    "* Second model(BPEmbed):---------accuracy 0.9794 , evaluation of test data [1.0082, 0.7649]\n",
    "* Third model(fastText):----------------accuracy 0.9851 , evaluation of test data [0.9825, 0.8019]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
